{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_sentiment_shuffled.txt\", 'r') as f:\n",
    "    lines = list(map(lambda l: l.split(\" \"), f.readlines()))\n",
    "    training_lines = lines[0: int(.64 * len(lines))]\n",
    "    #Distribution of Positive and Negative\n",
    "    cnt_class = dict(Counter([l[1] for l in training_lines]))\n",
    "    fig, (ax1) = plt.subplots(1,1, figsize=(5, 7.5))\n",
    "    ax1.bar(cnt_class.keys(), cnt_class.values(), align='center', width=.5)\n",
    "    ax1.set_title(\"Class vs Document Frequency\", fontsize=15)\n",
    "    ax1.set_xlabel(\"Class\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Document Frequency\", fontsize=15)\n",
    "    ax1.tick_params(axis=\"both\", labelsize=10, rotation=90)\n",
    "    plt.savefig(\"class_document_frequency.jpg\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with open(\"all_sentiment_shuffled.txt\", 'r') as f:\n",
    "    lines = list(map(lambda l: l.split(\" \"), f.readlines()))\n",
    "    training_lines = lines[0: int(.64 * len(lines))]\n",
    "    #Distribution of Topics\n",
    "    cnt_topic = dict(Counter([l[0] for l in training_lines]))\n",
    "    fig, (ax1) = plt.subplots(1,1, figsize=(10, 7.5))\n",
    "    ax1.bar(cnt_topic.keys(), cnt_topic.values(), align='center', width=.5)\n",
    "    ax1.set_title(\"Topic vs Document Frequency\", fontsize=15)\n",
    "    ax1.set_xlabel(\"Topic\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Document Frequency\", fontsize=15)\n",
    "    ax1.tick_params(axis=\"both\", labelsize=10, rotation=90)\n",
    "    plt.savefig(\"topic_document_frequency.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"all_sentiment_shuffled.txt\", 'r') as f:\n",
    "    lines = list(map(lambda l: l.split(\" \"), f.readlines()))\n",
    "    training_lines = lines[0: int(.64 * len(lines))]\n",
    "\n",
    "    #Distribution of Positive and Negative per topic class\n",
    "    cnt_topic_class = dict(Counter(l[0] + \"_\" + l[1] for l in training_lines))\n",
    "    cnt_topic_class_keys = sorted(cnt_topic_class.keys())\n",
    "    cnt_topic_class_values = []\n",
    "\n",
    "    for k in cnt_topic_class_keys:\n",
    "        cnt_topic_class_values.append(cnt_topic_class[k])\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1,1, figsize=(10, 7.5))\n",
    "    ax1.bar(cnt_topic_class_keys, cnt_topic_class_values, align='center', width=.5)\n",
    "    ax1.set_title(\"Topic_Sentiment vs Document Frequency\", fontsize=15)\n",
    "    ax1.set_xlabel(\"Topic_Sentiment\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Document Frequency\", fontsize=15)\n",
    "    ax1.tick_params(axis=\"both\", labelsize=10, rotation=90)\n",
    "    plt.savefig(\"class_topic_document_frequency.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "with open(\"all_sentiment_shuffled.txt\", 'r') as f:\n",
    "    lines = list(map(lambda l: l.split(\" \"), f.readlines()))\n",
    "    training_lines = lines[0: int(.64 * len(lines))]\n",
    "    \n",
    "    all_doc_lengths = [len(l[3: len(l)-2]) for l in training_lines]\n",
    "    min_document_length = min(all_doc_lengths)\n",
    "    max_document_length = max(all_doc_lengths)\n",
    "    median_document_length = int(statistics.median(all_doc_lengths))\n",
    "    mean_document_length = int(statistics.mean(all_doc_lengths))\n",
    "    mode_document_length = int(statistics.mode(all_doc_lengths))\n",
    "    standard_dev_document_length = int(statistics.stdev(all_doc_lengths))\n",
    "    print(f\"Document Length across all classes:\")\n",
    "    print(f\"The smallest document length is: {min_document_length}\")\n",
    "    print(f\"The largest document length is: {max_document_length}\")\n",
    "    print(f\"The median document length is: {median_document_length}\")\n",
    "    print(f\"The mean document length is: {mean_document_length}\")\n",
    "    print(f\"The mode document length is: {mode_document_length}\")\n",
    "    print(f\"The standard deviation in document length is: {standard_dev_document_length}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    neg_doc_length = [len(l[3: len(l)-2]) for l in training_lines if l[1] == 'neg']\n",
    "    neg_min_document_length = min(neg_doc_length)\n",
    "    neg_max_document_length = max(neg_doc_length)\n",
    "    neg_median_document_length = int(statistics.median(neg_doc_length))\n",
    "    neg_mean_document_length = int(statistics.mean(neg_doc_length))\n",
    "    neg_mode_document_length = int(statistics.mode(neg_doc_length))\n",
    "    neg_standard_dev_document_length = int(statistics.stdev(neg_doc_length))\n",
    "    print(f\"Document Length across Negative Class:\")\n",
    "    print(f\"The smallest negative document length is: {neg_min_document_length}\")\n",
    "    print(f\"The largest negative document length is: {neg_max_document_length}\")\n",
    "    print(f\"The median negative document length is: {neg_median_document_length}\")\n",
    "    print(f\"The mean negative document length is: {neg_mean_document_length}\")\n",
    "    print(f\"The mode negative document length is: {neg_mode_document_length}\")\n",
    "    print(f\"The standard deviation in negative document length is: {neg_standard_dev_document_length}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    pos_doc_length = [len(l[3: len(l)-2]) for l in training_lines if l[1] == 'pos']\n",
    "    pos_min_document_length = min(pos_doc_length)\n",
    "    pos_max_document_length = max(pos_doc_length)\n",
    "    pos_median_document_length = int(statistics.median(pos_doc_length))\n",
    "    pos_mean_document_length = int(statistics.mean(pos_doc_length))\n",
    "    pos_mode_document_length = int(statistics.mode(pos_doc_length))\n",
    "    pos_standard_dev_document_length = int(statistics.stdev(pos_doc_length))\n",
    "    print(f\"Document Length across Positive Class:\")\n",
    "    print(f\"The smallest positive document length is: {pos_min_document_length}\")\n",
    "    print(f\"The largest positive document length is: {pos_max_document_length}\")\n",
    "    print(f\"The median positive document length is: {pos_median_document_length}\")\n",
    "    print(f\"The mean positive document length is: {pos_mean_document_length}\")\n",
    "    print(f\"The mode positive document length is: {pos_mode_document_length}\")\n",
    "    print(f\"The standard deviation in positive document length is: {pos_standard_dev_document_length}\")\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Distribution of Document length\n",
    "cnt_document_length = Counter(all_doc_lengths)\n",
    "#Distribution of Document length(Negative)\n",
    "neg_cnt_document_length = Counter(neg_doc_length)\n",
    "#Distribution of Document length(Positive)\n",
    "pos_cnt_document_length = Counter(pos_doc_length)\n",
    "\n",
    "    \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(45, 45))\n",
    "ax1.bar(cnt_document_length.keys(), cnt_document_length.values(), align='center', width=7.5)\n",
    "ax1.set_title(\"Document Length vs Document Frequency\", fontsize=30)\n",
    "ax1.set_xlabel(\"Document Length\", fontsize=30)\n",
    "ax1.set_ylabel(\"Document Frequency\", fontsize=30)\n",
    "ax1.tick_params(axis=\"both\", labelsize=20)\n",
    "    \n",
    "ax2.bar(neg_cnt_document_length.keys(), neg_cnt_document_length.values(), align='center', width=7.5)\n",
    "ax2.set_title(\"Document Length vs Negative Document Frequency\", fontsize=30)\n",
    "ax2.set_xlabel(\"Document Length\", fontsize=30)\n",
    "ax2.set_ylabel(\"Negative Document Frequency\", fontsize=30)\n",
    "ax2.tick_params(axis=\"both\", labelsize=20)\n",
    "    \n",
    "ax3.bar(pos_cnt_document_length.keys(), pos_cnt_document_length.values(), align='center', width=7.5)\n",
    "ax3.set_title(\"Document Length vs Positive Document Frequency\", fontsize=30)\n",
    "ax3.set_xlabel(\"Document Length\", fontsize=30)\n",
    "ax3.set_ylabel(\"Positive Document Frequency\", fontsize=30)\n",
    "ax3.tick_params(axis=\"both\", labelsize=20)\n",
    "plt.savefig(\"document_length_distribution.jpg\")\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "with open(\"all_sentiment_shuffled.txt\", 'r') as f:\n",
    "    lines = list(map(lambda l: l.split(\" \"), f.readlines()))\n",
    "    training_lines = lines[0: int(.64 * len(lines))] \n",
    "    neg_training_lines = [l[3:len(l)-2] for l in training_lines if l[1]=='neg']\n",
    "    pos_training_lines = [l[3:len(l)-2] for l in training_lines if l[1]=='pos']\n",
    "    neg_training_lines = [list(dict.fromkeys(l)) for l in neg_training_lines]\n",
    "    pos_training_lines = [list(dict.fromkeys(l)) for l in pos_training_lines]\n",
    "    neg_words = [item for line in neg_training_lines for item in line]\n",
    "    pos_words = [item for line in pos_training_lines for item in line]\n",
    "    cnt_neg_words =dict(Counter(neg_words))\n",
    "    cnt_pos_words =dict(Counter(pos_words))\n",
    "    \n",
    "\n",
    "    neg_words_set = set(cnt_neg_words.keys())\n",
    "    pos_words_set = set(cnt_pos_words.keys())\n",
    "    all_words_set = neg_words_set.union(pos_words_set)\n",
    "    print(f\"Document Vocabulary Size: {len(all_words_set)}\")\n",
    "    print(f\"Negative Document Vocabulary Size: {len(neg_words_set)}\")\n",
    "    print(f\"Positive Document Vocabulary Size: {len(pos_words_set)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    unique_words_set = neg_words_set.symmetric_difference(pos_words_set)\n",
    "    same_words_set = neg_words_set.intersection(pos_words_set) \n",
    "    neg_unique_words = neg_words_set.difference(same_words_set)\n",
    "    pos_unique_words = pos_words_set.difference(same_words_set)\n",
    "    \n",
    "    print(f\"Common Words: {len(same_words_set)}\")\n",
    "    print(f\"Unique Words in Negative Documents: {len(neg_unique_words)}\")\n",
    "    print(f\"Unique Words in Positive Documents: {len(pos_unique_words)}\")\n",
    "    \n",
    "    cnt_unique_neg_words = {k:cnt_neg_words[k] for k in neg_unique_words}\n",
    "    cnt_unique_pos_words = {k:cnt_pos_words[k] for k in pos_unique_words}\n",
    "    \n",
    "    unique_neg_word_freq = dict(Counter(list(cnt_unique_neg_words.values())))\n",
    "    unique_pos_word_freq = dict(Counter(list(cnt_unique_pos_words.values())))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(30,7))\n",
    "    ax1.bar(unique_neg_word_freq.keys(), unique_neg_word_freq.values(), align='center', width=.5)\n",
    "    ax1.set_title(\"Number of Unique Words vs Nos of Negative Documents\", fontsize=15)\n",
    "    ax1.set_xlabel(\"Number of Unique Words\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Nos of Negative Documents\", fontsize=15)\n",
    "    ax1.tick_params(axis=\"both\", labelsize=20)\n",
    "    \n",
    "    ax2.bar(unique_pos_word_freq.keys(), unique_pos_word_freq.values(), align='center', width=.5)\n",
    "    ax2.set_title(\"Number of Unique Words vs Nos of Positive Documents\", fontsize=15)\n",
    "    ax2.set_xlabel(\"Number of Unique Words\", fontsize=15)\n",
    "    ax2.set_ylabel(\"Nos of Positive Documents\", fontsize=15)\n",
    "    ax2.tick_params(axis=\"both\", labelsize=20)\n",
    "    plt.savefig(\"Uniqness.jpg\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import joblib\n",
    "nb_correct = joblib.load('nb_correct.pkl')\n",
    "nb_correct_docs = [l[0].split(\" \") for l in nb_correct]\n",
    "\n",
    "nb_incorrect = joblib.load('nb_incorrect.pkl')\n",
    "nb_incorrect_docs = [l[0].split(\" \") for l in nb_incorrect]\n",
    "\n",
    "nb_incorrect_neg = [set(l[0].split(\" \")) for l in nb_incorrect if l[1]==\"neg\"]\n",
    "nb_incorrect_pos = [set(l[0].split(\" \")) for l in nb_incorrect if l[1]==\"pos\"]\n",
    "intersect_incorrect_uniq_neg_neg = [len(neg_unique_words.intersection(l)) for l in nb_incorrect_neg]\n",
    "intersect_incorrect_uniq_pos_neg = [len(pos_unique_words.intersection(l)) for l in nb_incorrect_neg]\n",
    "intersect_incorrect_uniq_neg_pos = [len(neg_unique_words.intersection(l)) for l in nb_incorrect_pos]\n",
    "intersect_incorrect_uniq_pos_pos = [len(pos_unique_words.intersection(l)) for l in nb_incorrect_pos]\n",
    "print(len(nb_incorrect_neg))\n",
    "print(len(nb_incorrect_pos))\n",
    "\n",
    "\n",
    "#UniqNeg_Neg\n",
    "unn = Counter(intersect_incorrect_uniq_neg_neg)\n",
    "#UniqPos_Neg\n",
    "upn = Counter(intersect_incorrect_uniq_pos_neg)\n",
    "#UniqNeg_Pos\n",
    "unp = Counter(intersect_incorrect_uniq_neg_pos)\n",
    "#UniqPos_Pos\n",
    "upp = Counter(intersect_incorrect_uniq_pos_pos)\n",
    "\n",
    "\n",
    "    \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(45, 45))\n",
    "ax1.bar(unn.keys(), unn.values(), align='center', width=.5)\n",
    "ax1.set_title(\"Intersection with Negative vs Negative Document Frequency\", fontsize=30)\n",
    "ax1.set_xlabel(\"Intersection with Negative\", fontsize=30)\n",
    "ax1.set_ylabel(\"Negative Document Frequency\", fontsize=30)\n",
    "ax1.tick_params(axis=\"both\", labelsize=20)\n",
    "    \n",
    "ax2.bar(unp.keys(), unp.values(), align='center', width=.5)\n",
    "ax2.set_title(\"Intersection with Positive vs Negative Document Frequency\", fontsize=30)\n",
    "ax2.set_xlabel(\"Intersection with Positive\", fontsize=30)\n",
    "ax2.set_ylabel(\"Negative Document Frequency\", fontsize=30)\n",
    "ax2.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "ax3.bar(unp.keys(), unp.values(), align='center', width=.5)\n",
    "ax3.set_title(\"Intersection with Negative vs Positive Document Frequency\", fontsize=30)\n",
    "ax3.set_xlabel(\"Intersection with Negative\", fontsize=30)\n",
    "ax3.set_ylabel(\"Positive Document Frequency\", fontsize=30)\n",
    "ax3.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "ax4.bar(upp.keys(), upp.values(), align='center', width=.5)\n",
    "ax4.set_title(\"Intersection with Positive vs Positive Document Frequency\", fontsize=30)\n",
    "ax4.set_xlabel(\"Intersection with Positive\", fontsize=30)\n",
    "ax4.set_ylabel(\"Positive Document Frequency\", fontsize=30)\n",
    "ax4.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "plt.savefig(\"incorrect_interesections.jpg\")    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in nb_incorrect[0:10]:\n",
    "    f = open(\"nb_incorrect_10.txt\", \"a\")\n",
    "    f.write(f\"Text: {i[0]}\")\n",
    "    f.write(f\"Actual Category: {i[1]}\")\n",
    "    f.write(f\"Probabilities: {i[2][0]}, {i[2][1]}\\n'\")\n",
    "    words = set(i[0].split(\" \"))\n",
    "    f.write(f\"Intersection with Unique Neg: {neg_unique_words.intersection(words)}\\n\")\n",
    "    f.write(f\"Intersection with Unique Pos: {pos_unique_words.intersection(words)}\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_correct_neg = [set(l[0].split(\" \")) for l in nb_correct if l[1]==\"neg\"]\n",
    "nb_correct_pos = [set(l[0].split(\" \")) for l in nb_correct if l[1]==\"pos\"]\n",
    "intersect_incorrect_uniq_neg_neg = [len(neg_unique_words.intersection(l)) for l in nb_correct_neg]\n",
    "intersect_incorrect_uniq_pos_neg = [len(pos_unique_words.intersection(l)) for l in nb_correct_neg]\n",
    "intersect_incorrect_uniq_neg_pos = [len(neg_unique_words.intersection(l)) for l in nb_correct_pos]\n",
    "intersect_incorrect_uniq_pos_pos = [len(pos_unique_words.intersection(l)) for l in nb_correct_pos]\n",
    "print(len(nb_incorrect_neg))\n",
    "print(len(nb_incorrect_pos))\n",
    "\n",
    "\n",
    "#UniqNeg_Neg\n",
    "unn = Counter(intersect_incorrect_uniq_neg_neg)\n",
    "#UniqPos_Neg\n",
    "upn = Counter(intersect_incorrect_uniq_pos_neg)\n",
    "#UniqNeg_Pos\n",
    "unp = Counter(intersect_incorrect_uniq_neg_pos)\n",
    "#UniqPos_Pos\n",
    "upp = Counter(intersect_incorrect_uniq_pos_pos)\n",
    "\n",
    "\n",
    "    \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(45, 45))\n",
    "ax1.bar(unn.keys(), unn.values(), align='center', width=.5)\n",
    "ax1.set_title(\"Intersection with Negative vs Negative Document Frequency\", fontsize=30)\n",
    "ax1.set_xlabel(\"Intersection with Negative\", fontsize=30)\n",
    "ax1.set_ylabel(\"Negative Document Frequency\", fontsize=30)\n",
    "ax1.tick_params(axis=\"both\", labelsize=20)\n",
    "    \n",
    "ax2.bar(unp.keys(), unp.values(), align='center', width=.5)\n",
    "ax2.set_title(\"Intersection with Positive vs Negative Document Frequency\", fontsize=30)\n",
    "ax2.set_xlabel(\"Intersection with Positive\", fontsize=30)\n",
    "ax2.set_ylabel(\"Negative Document Frequency\", fontsize=30)\n",
    "ax2.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "ax3.bar(unp.keys(), unp.values(), align='center', width=.5)\n",
    "ax3.set_title(\"Intersection with Negative vs Positive Document Frequency\", fontsize=30)\n",
    "ax3.set_xlabel(\"Intersection with Negative\", fontsize=30)\n",
    "ax3.set_ylabel(\"Positive Document Frequency\", fontsize=30)\n",
    "ax3.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "ax4.bar(upp.keys(), upp.values(), align='center', width=.5)\n",
    "ax4.set_title(\"Intersection with Positive vs Positive Document Frequency\", fontsize=30)\n",
    "ax4.set_xlabel(\"Intersection with Positive\", fontsize=30)\n",
    "ax4.set_ylabel(\"Positive Document Frequency\", fontsize=30)\n",
    "ax4.tick_params(axis=\"both\", labelsize=20)\n",
    "\n",
    "plt.savefig(\"correct_intersections.jpg\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata-lab",
   "language": "python",
   "name": "bigdata-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
